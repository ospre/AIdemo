{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Conversational Interface - Chat with Llama 3 and Titan Premier LLMs\n",
    "\n",
    "In this notebook, you build a chatbot using the llama3-8b-instruct and titan-text-premier Foundation Models (FMs) in Amazon Bedrock.\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can enhance the user experience for your customers. Chatbots use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. You can use chatbots in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. Users can access them through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "- **Chatbot (Basic)**, Zero Shot chatbot with a FM model\n",
    "- **Chatbot using prompt**, template(LangChain) - Chatbot with some context provided in the prompt template\n",
    "- **Chatbot with persona**, Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "- **Contextual-aware chatbot**, Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## LangChain framework for building Chatbot with Amazon Bedrock\n",
    "\n",
    "In conversational interfaces such as chatbots, remembering previous interactions becomes highly important, both at a short-term and long-term level.\n",
    "\n",
    "The LangChain framework provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular. Secondly, LangChain provides easy ways to incorporate these utilities into chains, allowing you to easily define and interact with different types of abstractions, which make powerful chatbots easy for you to build.\n",
    "\n",
    "## Building a Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in building a context-aware chatbot is to generate embeddings for the context. Typically, you have an ingestion process which runs through your embedding model and generates the embeddings, which will then be stored in a vector store. In this notebook, you use the Titan Embeddings model for this. The second process is user request orchestration, interaction, invoking, and returning the results. This involves orchestrating the user request, interacting with the necessary models/components to gather information, invoking the chatbot to formulate a response, and then returning the chatbot's response back to the user.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 4.2: Using CoversationChain from LangChain to start the conversation\n",
    "\n",
    "In this task, you enable the chatbot to carry conversational context across multiple interactions with users. Having a conversational memory is crucial for Chatbots to hold meaningful, coherent dialogues over time.\n",
    "\n",
    "You implement conversational memory capabilities by building on top of LangChain's ConversationChain class. The ConversationChain acts as a persistent memory module, allowing to store and retrieve historical statements made by both the user and the chatbot agent.\n",
    "\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The model outputs are non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Hi there! <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Ah, hello! It's great to chat with you. I'm an AI designed to assist and communicate with humans. I've been trained on a vast amount of text data, which allows me to understand and respond to a wide range of questions and topics. I'm constantly learning and improving my abilities, so please bear with me if I make any mistakes. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "#model configuration\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "titan_llm = BedrockLLM(model_id=\"amazon.titan-text-premier-v1:0\", client=bedrock_client, model_kwargs={\n",
    "    \"maxTokenCount\":512,\n",
    "    \"stopSequences\":[],\n",
    "    \"temperature\":0,\n",
    "    \"topP\":0.9\n",
    "    })\n",
    "\n",
    "llama3_llm = BedrockLLM(model_id=\"meta.llama3-8b-instruct-v1:0\", client=bedrock_client, model_kwargs={\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1,\n",
    "    })\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llama3_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Hi there!\"}]\n",
    "\n",
    "try:\n",
    "    print(conversation.predict(input=format_instructions(instructions)))\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Questions\n",
    "\n",
    "The model has responded with an initial message. Now, you ask it a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Hi there! <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:  Ah, hello! It's great to chat with you. I'm an AI designed to assist and communicate with humans. I've been trained on a vast amount of text data, which allows me to understand and respond to a wide range of questions and topics. I'm constantly learning and improving my abilities, so please bear with me if I make any mistakes. What would you like to talk about?\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Give me a few tips on how to start a new garden. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "  Starting a new garden! Exciting! I'd be happy to help. Here are a few tips to get you started:\n",
      "\n",
      "First, choose a location that gets at least 6 hours of direct sunlight a day. Most vegetables and flowers need plenty of sunlight to grow well.\n",
      "\n",
      "Next, prepare the soil. You can test the pH level of your soil using a DIY kit or send a sample to a lab for analysis. Most plants prefer a slightly acidic to neutral soil pH, between 6.0 and 7.0. If your soil is too alkaline, you can add elemental sulfur or peat moss to lower the pH. If it's too acidic, you can add lime to raise the pH.\n",
      "\n",
      "Before planting, remove any debris, rocks, and weeds from the area. You can also till the soil to loosen it and create a smooth surface. If your soil is heavy clay or sandy, you may want to add organic matter like compost or well-rotted manure to improve its structure.\n",
      "\n",
      "When selecting plants, consider the climate and growing conditions in your area. Check the specific requirements for each plant, such as temperature, moisture, and sunlight needs.\n",
      "\n",
      "Finally, start small and don't be afraid to experiment. You can always add more plants or try new varieties as you gain experience.\n",
      "\n",
      "Would you like more specific advice on a particular aspect of gardening?\n"
     ]
    }
   ],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Give me a few tips on how to start a new garden.\"}]\n",
    "print(conversation.predict(input=format_instructions(instructions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build on the questions\n",
    "\n",
    "Now, ask a question without mentioning the word garden to see if the model can understand the previous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Hi there! <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:  Ah, hello! It's great to chat with you. I'm an AI designed to assist and communicate with humans. I've been trained on a vast amount of text data, which allows me to understand and respond to a wide range of questions and topics. I'm constantly learning and improving my abilities, so please bear with me if I make any mistakes. What would you like to talk about?\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Give me a few tips on how to start a new garden. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:   Starting a new garden! Exciting! I'd be happy to help. Here are a few tips to get you started:\n",
      "\n",
      "First, choose a location that gets at least 6 hours of direct sunlight a day. Most vegetables and flowers need plenty of sunlight to grow well.\n",
      "\n",
      "Next, prepare the soil. You can test the pH level of your soil using a DIY kit or send a sample to a lab for analysis. Most plants prefer a slightly acidic to neutral soil pH, between 6.0 and 7.0. If your soil is too alkaline, you can add elemental sulfur or peat moss to lower the pH. If it's too acidic, you can add lime to raise the pH.\n",
      "\n",
      "Before planting, remove any debris, rocks, and weeds from the area. You can also till the soil to loosen it and create a smooth surface. If your soil is heavy clay or sandy, you may want to add organic matter like compost or well-rotted manure to improve its structure.\n",
      "\n",
      "When selecting plants, consider the climate and growing conditions in your area. Check the specific requirements for each plant, such as temperature, moisture, and sunlight needs.\n",
      "\n",
      "Finally, start small and don't be afraid to experiment. You can always add more plants or try new varieties as you gain experience.\n",
      "\n",
      "Would you like more specific advice on a particular aspect of gardening?\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "bugs <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "  Ah, bugs! Yes, bugs can be a challenge in gardening. Here are a few tips to help you manage common garden pests:\n",
      "\n",
      "First, keep your garden clean. Remove any debris, weeds, and dead plants to reduce hiding spots for pests. Regularly inspect your plants for signs of infestation, such as holes in leaves, eggs, or actual bugs.\n",
      "\n",
      "For aphids, whiteflies, and other soft-bodied insects, try using neem oil or insecticidal soap. These gentle pesticides can be applied directly to the affected areas.\n",
      "\n",
      "For slugs and snails, try using copper tape or crushed eggshells around the perimeter of your plants. These can deter them from crawling up the stems.\n",
      "\n",
      "For caterpillars, hand-picking is often the most effective method. Simply pluck the caterpillars off the plants and dispose of them. You can also use Bt (Bacillus thuringiensis) or spinosad to control caterpillar populations.\n",
      "\n",
      "For ants, try using cinnamon or cayenne pepper around the affected areas. Ants don't like the smell of these spices and will avoid them.\n",
      "\n",
      "For Japanese beetles, try using a beetle trap or hand-picking them off the plants. You can also use pyrethrin or permethrin to control infestations.\n",
      "\n",
      "Remember, it's essential to identify the pest correctly before applying any control methods. Misidentification can lead to ineffective or even harmful treatments.\n",
      "\n",
      "Would you like more information on a specific type of bug or pest control method?\n",
      "Human: \n"
     ]
    }
   ],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"bugs\"}]\n",
    "print(conversation.predict(input=format_instructions(instructions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing this conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Hi there! <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:  Ah, hello! It's great to chat with you. I'm an AI designed to assist and communicate with humans. I've been trained on a vast amount of text data, which allows me to understand and respond to a wide range of questions and topics. I'm constantly learning and improving my abilities, so please bear with me if I make any mistakes. What would you like to talk about?\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Give me a few tips on how to start a new garden. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:   Starting a new garden! Exciting! I'd be happy to help. Here are a few tips to get you started:\n",
      "\n",
      "First, choose a location that gets at least 6 hours of direct sunlight a day. Most vegetables and flowers need plenty of sunlight to grow well.\n",
      "\n",
      "Next, prepare the soil. You can test the pH level of your soil using a DIY kit or send a sample to a lab for analysis. Most plants prefer a slightly acidic to neutral soil pH, between 6.0 and 7.0. If your soil is too alkaline, you can add elemental sulfur or peat moss to lower the pH. If it's too acidic, you can add lime to raise the pH.\n",
      "\n",
      "Before planting, remove any debris, rocks, and weeds from the area. You can also till the soil to loosen it and create a smooth surface. If your soil is heavy clay or sandy, you may want to add organic matter like compost or well-rotted manure to improve its structure.\n",
      "\n",
      "When selecting plants, consider the climate and growing conditions in your area. Check the specific requirements for each plant, such as temperature, moisture, and sunlight needs.\n",
      "\n",
      "Finally, start small and don't be afraid to experiment. You can always add more plants or try new varieties as you gain experience.\n",
      "\n",
      "Would you like more specific advice on a particular aspect of gardening?\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "bugs <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:   Ah, bugs! Yes, bugs can be a challenge in gardening. Here are a few tips to help you manage common garden pests:\n",
      "\n",
      "First, keep your garden clean. Remove any debris, weeds, and dead plants to reduce hiding spots for pests. Regularly inspect your plants for signs of infestation, such as holes in leaves, eggs, or actual bugs.\n",
      "\n",
      "For aphids, whiteflies, and other soft-bodied insects, try using neem oil or insecticidal soap. These gentle pesticides can be applied directly to the affected areas.\n",
      "\n",
      "For slugs and snails, try using copper tape or crushed eggshells around the perimeter of your plants. These can deter them from crawling up the stems.\n",
      "\n",
      "For caterpillars, hand-picking is often the most effective method. Simply pluck the caterpillars off the plants and dispose of them. You can also use Bt (Bacillus thuringiensis) or spinosad to control caterpillar populations.\n",
      "\n",
      "For ants, try using cinnamon or cayenne pepper around the affected areas. Ants don't like the smell of these spices and will avoid them.\n",
      "\n",
      "For Japanese beetles, try using a beetle trap or hand-picking them off the plants. You can also use pyrethrin or permethrin to control infestations.\n",
      "\n",
      "Remember, it's essential to identify the pest correctly before applying any control methods. Misidentification can lead to ineffective or even harmful treatments.\n",
      "\n",
      "Would you like more information on a specific type of bug or pest control method?\n",
      "Human: \n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "That's all, thank you! <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   You're welcome! It was a pleasure chatting with you and helping with your gardening questions. If you have any more questions or need further assistance in the future, don't hesitate to reach out. Happy gardening!\n"
     ]
    }
   ],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"That's all, thank you!\"}]\n",
    "print(conversation.predict(input=format_instructions(instructions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Chatbot using prompt template (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you use the default PromptTemplate that is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:DEFAULT:PROMPT:TEMPLATE: is =The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "#default prompt template\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "qa= ConversationChain(\n",
    "    llm=llama3_llm, verbose=False, memory=ConversationBufferMemory() #memory_chain\n",
    ")\n",
    "\n",
    "print(f\"ChatBot:DEFAULT:PROMPT:TEMPLATE: is ={qa.prompt.template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.invoke({'question': prompt})\n",
    "                    else:\n",
    "                        result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3cac6ce8b0459697b5c88ae9fe2d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start chat\n",
    "chat = ChatUX(qa)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 4.4: Chatbot with persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, Artificial Intelligence(AI) assistant plays the role of a career coach. Role play dialogue requires pre-populating the conversation with a user message before starting the chat. A ConversationBufferMemory is used to pre-populate the dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "Context: You will be acting as a career coach. Your goal is to give career advice to users. For questions that are not career related, don't provide advice. Say, I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "What are the career options in AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Ah, what a great question! As a career coach, I'm excited to share with you the numerous career options in AI. With the rapid growth of AI and machine learning, the job market is evolving rapidly. Here are some of the most in-demand roles:\n",
      "\n",
      "1. **Machine Learning Engineer**: Design and develop intelligent systems that can learn from data and improve over time.\n",
      "2. **Data Scientist**: Extract insights and patterns from large datasets to inform business decisions.\n",
      "3. **AI Researcher**: Conduct research in AI and machine learning to develop new algorithms and techniques.\n",
      "4. **Natural Language Processing (NLP) Specialist**: Develop AI systems that can understand, generate, and process human language.\n",
      "5. **Computer Vision Engineer**: Design and develop AI systems that can interpret and understand visual data from images and videos.\n",
      "6. **Robotics Engineer**: Design and develop intelligent robots that can interact with their environment and perform tasks autonomously.\n",
      "7. **AI Ethics Specialist**: Ensure that AI systems are developed and used in an ethical and responsible manner.\n",
      "8. **AI Developer**: Build and deploy AI models and systems for various industries, such as healthcare, finance, and education.\n",
      "9. **AI Consultant**: Help organizations implement AI solutions and strategies to improve their operations and decision-making.\n",
      "10. **AI Trainer**: Train AI models on large datasets and fine-tune their performance for specific applications.\n",
      "\n",
      "These are just a few examples of the many career options in AI. As the field continues to evolve, new roles and specializations will emerge. If you're interested in pursuing a career in AI, I recommend developing skills in programming languages like Python, R, or Julia, as well as machine learning frameworks like TensorFlow or PyTorch.\n",
      "\n",
      "What specific area of AI interests you the most?\n"
     ]
    }
   ],
   "source": [
    "# conversation context as a career coach\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(format_prompt(\"user\",\"Context: You will be acting as a career coach. Your goal is to give career advice to users. For questions that are not career related, don't provide advice. Say, I don't know.\"))\n",
    "conversation = ConversationChain(\n",
    "     llm=llama3_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=format_prompt(\"user\",\"What are the career options in AI?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ask a question that is not within this persona's specialty. The model should not answer that question and should give a reason for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I'm a career coach, not a mechanic! I don't have the expertise to provide advice on fixing cars. I can suggest searching online for tutorials, consulting a professional mechanic, or checking your car's manual for guidance. If you have a specific question about a career-related topic, I'd be happy to help. Otherwise, I'm not the right AI for the job.\n"
     ]
    }
   ],
   "source": [
    "# question with different context\n",
    "conversation.verbose = False\n",
    "print(conversation.predict(input=format_prompt(\"user\",\"How to fix my car?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.5 Chatbot with Context\n",
    "\n",
    "In this task, you ask the chatbot to answer questions based on context that was passed to it. You take a CSV file and use the Titan embeddings model to create a vector representing that context. This vector is stored in Facebook AI Similarity Search (FAISS). When the chatbot is asked a question, you will pass this vector back to the chatbot and have it retrieve the answer using the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titan embeddings Model\n",
    "\n",
    "Embeddings represent words, phrases, or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "You use embeddings for the Retrieval-Augmented Generation (RAG) [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS as VectorStore\n",
    "\n",
    "In order to use embeddings for search, you need a store that can efficiently perform vector similarity searches. In this notebook, you use FAISS, which is an in-memory store. To permanently store vectors, you can use Knowledge Bases for Amazon Bedrock, pgVector, Pinecone, Weaviate, or Chroma.\n",
    "\n",
    "The LangChain VectorStore APIs are available [here](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:loaded:size=153\n",
      "Documents:after split and chunking size=154\n",
      "vectorstore_faiss_aws:created=<langchain_community.vectorstores.faiss.FAISS object at 0x7f61b84b5210>::\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a quick low code test \n",
    "\n",
    "You can use a Wrapper class provided by LangChain to query the vector database store and return the relevant documents. This runs a QA Chain with all default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R is supported with Amazon SageMaker. You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, enabling ML practitioners to build, train, tune, and deploy R models.\n"
     ]
    }
   ],
   "source": [
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"R in SageMaker\", llm=titan_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot application\n",
    "\n",
    "For the chatbot, you need context management, history, vector stores, and many other components. You start by building a ConversationalRetrievalChain.\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain, which allow for passing in chat history that can be used for follow-up questions.\n",
    "\n",
    "Set verbose to `True` to see everything going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass chat history for follow-up questions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters used for ConversationRetrievalChain\n",
    "\n",
    "- **Retriever:** You used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types to choose from: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object, where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "- **Memory:** A memory chain stores the history.\n",
    "\n",
    "- **Condense_question_prompt:** Given a user's question, you use the previous conversation and that question to construct a standalone question.\n",
    "\n",
    "- **Chain_type:** If the chat history is long and exceeds the context limit, set this parameter to stuff, refine, map_reduce, or map-rerank.\n",
    "\n",
    "If the question asked falls outside the scope of the context, the model will reply that it doesn't know the answer.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** To understand how the chain functions, uncomment the verbose=True line. This will print out detailed logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=titan_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    chain_type='stuff', \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}:\n",
    "\n",
    "Use at maximum 3 sentences to answer the question.If the context does not contain information to answer the\n",
    "question, please state that \"I could not find an exact answer to the question.\"\n",
    "\n",
    "{question}:\n",
    "\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad43e5d780c45a39a961c6638a30c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have utilized Titan LLM to create a conversational interface with following patterns:\n",
    "\n",
    "- Chatbot (Basic - without context)\n",
    "- Chatbot using prompt template(Langchain)\n",
    "- Chatbot with personas\n",
    "- Chatbot with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task 5**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:850833645101:studio-lifecycle-config/lcc-kernel"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
